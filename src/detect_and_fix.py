import os
import tensorflow as tf
from preprocess import tokenize_data
from model import build_model
import re
import pandas as pd

# Constants
INPUT_SIZE = 500
VOCAB_SIZE = 10000
MODEL_PATH = '../saved_models/model.weights.h5'
TARGET_FOLDER = '../example.c/'  # ë¶„ì„í•  C íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ

# CWE ìˆ˜ì • ê¶Œì¥ì‚¬í•­
CWE_FIXES = {
    "CWE-120": "ë²„í¼ ì˜¤ë²„í”Œë¡œë¥¼ ë°©ì§€í•˜ë ¤ë©´ strcpy ëŒ€ì‹  strncpy ë˜ëŠ” snprintfë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
    "CWE-119": "ë²„í¼ ê²½ê³„ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ë°°ì—´ ì¸ë±ìŠ¤ë¥¼ ê²€ì‚¬í•˜ì„¸ìš”.",
    "CWE-469": "í¬ì¸í„° ê³„ì‚°ì— ì •í™•í•œ ë°ì´í„° í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
    "CWE-476": "NULL í¬ì¸í„°ë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì— í•­ìƒ NULL ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
    "CWE-other": "ì½”ë“œ ì „ì²´ë¥¼ ê²€í† í•˜ê³  ì…ë ¥ ê²€ì¦ ë˜ëŠ” ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ ì¶”ê°€í•˜ì„¸ìš”."
}

def load_model(input_size, vocab_size):
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    model = build_model(input_size, vocab_size)
    model.load_weights(MODEL_PATH)
    return model

def remove_comments(code):
    """
    C ì½”ë“œì—ì„œ ì£¼ì„ì„ ì œê±°í•©ë‹ˆë‹¤.
    Args:
        code (str): ì…ë ¥ëœ C ì½”ë“œ.
    Returns:
        str: ì£¼ì„ì´ ì œê±°ëœ C ì½”ë“œ.
    """
    # ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì„ ì œê±° (/* */ ë˜ëŠ” // ìŠ¤íƒ€ì¼)
    pattern = r'(\/\*[\s\S]*?\*\/)|(\/\/.*)'
    return re.sub(pattern, '', code)

def analyze_code(code, tokenizer, model):
    """
    ì…ë ¥ëœ ì½”ë“œì˜ ì·¨ì•½ì ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    Args:
        code (str): ë¶„ì„í•  C ì½”ë“œ.
        tokenizer (Tokenizer): Keras í† í¬ë‚˜ì´ì €.
        model (tf.keras.Model): í•™ìŠµëœ ëª¨ë¸.
    Returns:
        dict: ì·¨ì•½ì  íƒì§€ ê²°ê³¼ ë° ì¶”ì²œ ìˆ˜ì • ë°©ë²•.
    """
    # ì „ì²˜ë¦¬ ë° í† í°í™”
    tokenized_code = tokenize_data(tokenizer, pd.DataFrame({"functionSource": [code]}), INPUT_SIZE)

    # ì˜ˆì¸¡
    prediction = model.predict(tokenized_code)[0][0]
    print(f"  ğŸ” ì˜ˆì¸¡ í™•ë¥ : {prediction}")  # ì˜ˆì¸¡ í™•ë¥  ì¶œë ¥
    is_vulnerable = prediction > 0.1

    if not is_vulnerable:
        return {"vulnerable": False, "recommendations": []}

    # CWE íƒ€ì… ì¶”ì • ë° ìˆ˜ì • ê¶Œì¥ì‚¬í•­
    recommendations = []
    for cwe_id, fix in CWE_FIXES.items():
        if re.search(cwe_id, code, re.IGNORECASE):  # ì½”ë“œ ë‚´ CWE íŒ¨í„´ í™•ì¸
            recommendations.append({"CWE": cwe_id, "fix": fix})

    return {"vulnerable": True, "recommendations": recommendations}


def analyze_folder(folder_path, tokenizer, model):
    """
    ì§€ì •ëœ í´ë” ë‚´ ëª¨ë“  `.c` íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    Args:
        folder_path (str): ë¶„ì„í•  í´ë” ê²½ë¡œ.
        tokenizer (Tokenizer): Keras í† í¬ë‚˜ì´ì €.
        model (tf.keras.Model): í•™ìŠµëœ ëª¨ë¸.
    """
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.c'):
                file_path = os.path.abspath(os.path.join(root, file))
                print(f"\në¶„ì„ ì¤‘: {file_path}")

                # íŒŒì¼ ì½ê¸°
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        code = f.read()
                except Exception as e:
                    print(f"  âš ï¸ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue

                # ì½”ë“œ ë¶„ì„
                result = analyze_code(code, tokenizer, model)

                # ê²°ê³¼ ì¶œë ¥
                if not result["vulnerable"]:
                    print("  âœ… ì´ ì½”ë“œëŠ” ì·¨ì•½í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.")
                else:
                    print("  âš ï¸ ì·¨ì•½ì ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
                    for rec in result["recommendations"]:
                        print(f"  - CWE: {rec['CWE']}")
                        print(f"    ê¶Œì¥ ìˆ˜ì • ì‚¬í•­: {rec['fix']}")


def main():
    # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)
    tokenizer.fit_on_texts([""])  # ë¹ˆ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    model = load_model(INPUT_SIZE, VOCAB_SIZE)

    # í´ë” ë¶„ì„ ì‹¤í–‰
    if not os.path.exists(TARGET_FOLDER):
        print(f"í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {TARGET_FOLDER}")
    else:
        analyze_folder(TARGET_FOLDER, tokenizer, model)

if __name__ == "__main__":
    main()
